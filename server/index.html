<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>TortanaTortana</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link
      rel="stylesheet"
      type="text/css"
      href="https://bootswatch.com/4/cerulean/bootstrap.min.css"
    />
  </head>
  <body style="padding: 50px">
    <video id="videoPlayer" width="50%" controls>
      <source src="" type="audio/mp3" />
    </video>
    <h1>Tortana</h1>
    <div id="controls">
      <button id="recordButton">Record</button>
      <button id="transcribeButton" disabled>Stop and upload to server</button>
    </div>
    <div id="output"></div>
    <script src="https://cdn.rawgit.com/mattdiamond/Recorderjs/08e7abd9/dist/recorder.js"></script>
    <script>
      // Retrieve data from localStorage
      let fileName = localStorage.getItem('videoSource');

      if (fileName) {
        // creating the route path
        let videoSource = 'api/audio' + '/' + fileName;
        // Access the video player element
        let videoPlayer = document.getElementById('videoPlayer');

        // Update the src attribute with the retrieved data
        videoPlayer.src = videoSource;
      }

      let rec = null;
      let audioStream = null;

      const recordButton = document.getElementById('recordButton');
      const transcribeButton = document.getElementById('transcribeButton');

      recordButton.addEventListener('click', startRecording);
      transcribeButton.addEventListener('click', transcribeText);

      function startRecording() {
        let constraints = {audio: true, video: false};

        recordButton.disabled = true;
        transcribeButton.disabled = false;

        navigator.mediaDevices
          .getUserMedia(constraints)
          .then(function (stream) {
            const audioContext = new window.AudioContext();
            audioStream = stream;
            const input = audioContext.createMediaStreamSource(stream);
            rec = new Recorder(input, {numChannels: 1});
            rec.record();
            document.getElementById('output').innerHTML =
              'Recording started...';
          })
          .catch(function (err) {
            recordButton.disabled = false;
            transcribeButton.disabled = true;
          });
      }

      function transcribeText() {
        document.getElementById('output').innerHTML =
          'Converting audio to text...';
        transcribeButton.disabled = true;
        recordButton.disabled = false;
        rec.stop();
        audioStream.getAudioTracks()[0].stop();
        rec.exportWAV(uploadSoundData);
      }

      function uploadSoundData(blob) {
        const filename = 'sound-file-' + new Date().getTime() + '.wav';

        const formData = new FormData();
        formData.append('audio_data', blob, filename);

        fetch('http://localhost:6222/api/audio', {
          method: 'POST',
          body: formData,
        })
          .then(async (result) => {
            document.getElementById('output').innerHTML = await result.text();

            localStorage.setItem('videoSource', filename);

            // location.reload();
          })
          .catch((error) => {
            document.getElementById('output').innerHTML =
              'An error occurred: ' + error;
          });
      }
    </script>
  </body>
</html>
